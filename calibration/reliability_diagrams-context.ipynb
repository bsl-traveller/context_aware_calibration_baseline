{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle, glob\n",
    "\n",
    "from cal_methods import HistogramBinning, TemperatureScaling\n",
    "from betacal import BetaCalibration\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from unpickle_probs import unpickle_probs\n",
    "from evaluation import calibrationError, softmax\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_diagram_sub(accs, confs, bins, ax, M = 10, name = \"Reliability Diagram\", xname = \"\", yname=\"\"):\n",
    "    def autolabel(rects):\n",
    "        for idx, rect in enumerate(rects):\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * height,\n",
    "                    bins[idx],\n",
    "                    ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    acc_conf = np.column_stack([accs,confs])\n",
    "    acc_conf.sort(axis=1)\n",
    "    outputs = acc_conf[:, 0]\n",
    "    gap = acc_conf[:, 1]\n",
    "\n",
    "    bin_size = 1/M\n",
    "    positions = np.arange(0+bin_size/2, 1+bin_size/2, bin_size)\n",
    "\n",
    "    # Plot gap first, so its below everything\n",
    "    gap_plt = ax.bar(positions, gap, width = bin_size, edgecolor = \"red\", color = \"red\", alpha = 0.3, label=\"Gap\", linewidth=2, zorder=2)\n",
    "\n",
    "    autolabel(gap_plt)\n",
    "\n",
    "    # Next add error lines\n",
    "    #for i in range(M):\n",
    "        #plt.plot([i/M,1], [0, (M-i)/M], color = \"red\", alpha=0.5, zorder=1)\n",
    "\n",
    "    #Bars with outputs\n",
    "    output_plt = ax.bar(positions, outputs, width = bin_size, edgecolor = \"black\", color = \"blue\", label=\"Outputs\", zorder = 3)\n",
    "\n",
    "    # Line plot with center line.\n",
    "    ax.set_aspect('equal')\n",
    "    ax.plot([0,1], [0,1], linestyle = \"--\")\n",
    "    ax.legend(handles = [gap_plt, output_plt])\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_title(name, fontsize=24)\n",
    "    ax.set_xlabel(xname, fontsize=22, color = \"black\")\n",
    "    ax.set_ylabel(yname, fontsize=22, color = \"black\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_conf(y_probs, normalize=False):\n",
    "    y_preds = np.argmax(y_probs, axis=1)  # Take maximum confidence as prediction\n",
    "\n",
    "    if normalize:\n",
    "        y_confs = np.max(y_probs, axis=1) / np.sum(y_probs, axis=1)\n",
    "    else:\n",
    "        y_confs = np.max(y_probs, axis=1)  # Take only maximum confidence\n",
    "\n",
    "    return y_preds, y_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_on_samples(method, path, file, M=15, approach=\"single\", m_kwargs={}):\n",
    "    bin_size = 1 / M\n",
    "\n",
    "    FILE_PATH = os.path.join(path, file)\n",
    "    (y_logits_val, y_val), (y_logits_test, y_test) = unpickle_probs(FILE_PATH)\n",
    "\n",
    "    y_probs_val = softmax(y_logits_val)[:, :10]  # Softmax logits\n",
    "    y_probs_test = softmax(y_logits_test)[:, :10]\n",
    "    \n",
    "    if approach == \"single\":\n",
    "        model = dict()\n",
    "        K = y_probs_test.shape[1]\n",
    "\n",
    "        # Go through all the classes\n",
    "        for k in range(K):\n",
    "            # Prep class labels (1 fixed true class, 0 other classes)\n",
    "            y_cal = np.array(y_val == k, dtype=\"int\")[:, 0]\n",
    "\n",
    "            # Train model\n",
    "            model[k] = method(**m_kwargs)\n",
    "            model[k].fit(y_probs_val[:, k], y_cal)  # Get only one column with probs for given class \"k\"\n",
    "\n",
    "    else:\n",
    "        model = method(**m_kwargs)\n",
    "        model.fit(y_logits_val, y_val)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_res(model, path, file, M=15, approach=\"single\"):\n",
    "    bin_size = 1 / M\n",
    "\n",
    "    FILE_PATH = os.path.join(path, file)\n",
    "    (y_logits_val, y_val), (y_logits_test, y_test) = unpickle_probs(FILE_PATH)\n",
    "\n",
    "    y_probs_val = softmax(y_logits_val)[:, :10]  # Softmax logits\n",
    "    y_probs_test = softmax(y_logits_test)[:, :10]\n",
    "\n",
    "    if approach == \"single\":\n",
    "\n",
    "        K = y_probs_test.shape[1]\n",
    "\n",
    "        # Go through all the classes\n",
    "        for k in range(K):\n",
    "            # Prep class labels (1 fixed true class, 0 other classes)\n",
    "            y_cal = np.array(y_val == k, dtype=\"int\")[:, 0]\n",
    "\n",
    "            y_probs_val[:, k] = model[k].predict(y_probs_val[:, k])  # Predict new values based on the fittting\n",
    "            y_probs_test[:, k] = model[k].predict(y_probs_test[:, k])\n",
    "\n",
    "            # Replace NaN with 0, as it should be close to zero  # TODO is it needed?\n",
    "            idx_nan = np.where(np.isnan(y_probs_test))\n",
    "            y_probs_test[idx_nan] = 0\n",
    "\n",
    "            idx_nan = np.where(np.isnan(y_probs_val))\n",
    "            y_probs_val[idx_nan] = 0\n",
    "\n",
    "            y_preds_val, y_confs_val = get_pred_conf(y_probs_val, normalize=True)\n",
    "            y_preds_test, y_confs_test = get_pred_conf(y_probs_test, normalize=True)\n",
    "\n",
    "    else:\n",
    "        y_probs_val = model.predict(y_logits_val)\n",
    "        y_probs_test = model.predict(y_logits_test)\n",
    "\n",
    "        y_preds_val, y_confs_val = get_pred_conf(y_probs_val, normalize=False)\n",
    "        y_preds_test, y_confs_test = get_pred_conf(y_probs_test, normalize=False)\n",
    "        \n",
    "    errors_val = calibrationError(y_confs_val, y_preds_val, y_val, bin_size=bin_size)\n",
    "    _, _, bin_info_val = errors_val.calculate_errors()\n",
    "    \n",
    "    errors_test = calibrationError(y_confs_test, y_preds_test, y_test, bin_size=bin_size)\n",
    "    _, _, bin_info_test = errors_test.calculate_errors()\n",
    "    \n",
    "    return (bin_info_test[\"accuracies\"], bin_info_test[\"confidences\"], bin_info_test[\"bin_lengths\"]), (bin_info_val[\"accuracies\"], bin_info_val[\"confidences\"], bin_info_val[\"bin_lengths\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncalibrated_res(path, file, M=15):\n",
    "    bin_size = 1 / M\n",
    "\n",
    "    FILE_PATH = os.path.join(path, file)\n",
    "    (y_logits_val, y_val), (y_logits_test, y_test) = unpickle_probs(FILE_PATH)\n",
    "\n",
    "    y_probs_test = softmax(y_logits_test)\n",
    "    y_preds_test, y_confs_test = get_pred_conf(y_probs_test, normalize=False)\n",
    "    \n",
    "    errors = calibrationError(y_confs_test, y_preds_test, y_test, bin_size=bin_size)\n",
    "    _, _, bin_info = errors.calculate_errors()\n",
    "    \n",
    "    return bin_info[\"accuracies\"], bin_info[\"confidences\"], bin_info[\"bin_lengths\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plots(files, plot_names=[], M=15, val_set=False):\n",
    "    pickle_path = os.path.join(\"..\", \"pickle_dump\")\n",
    "    out_path = os.path.join(\"..\", \"reliability_plots\")\n",
    "    if val_set:  # Plot Reliability diagrams for validation set\n",
    "        k = 1\n",
    "    else:\n",
    "        k = 0\n",
    "        \n",
    "    original_file = [s for s in files if \"sample\" in s][0]\n",
    "    model_ts = calibrate_on_samples(TemperatureScaling, PATH, original_file, M, \"multi\")\n",
    "    model_hb = calibrate_on_samples(HistogramBinning, PATH, original_file, M, \"single\", {'M': M})\n",
    "    model_iso = calibrate_on_samples(IsotonicRegression, PATH, original_file, M, \"single\", {'y_min': 0, 'y_max': 1})\n",
    "    model_beta = calibrate_on_samples(BetaCalibration, PATH, original_file, M, \"single\", {'parameters': \"abm\"})\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        bin_info_uncal = get_uncalibrated_res(PATH, file, M)\n",
    "\n",
    "        accs_confs = []\n",
    "        \n",
    "        accs_confs.append(cal_res(model_ts, PATH, file, M, \"multi\"))\n",
    "        accs_confs.append(cal_res(model_hb, PATH, file, M, \"single\"))\n",
    "        accs_confs.append(cal_res(model_iso, PATH, file, M, \"single\"))\n",
    "        accs_confs.append(cal_res(model_beta, PATH, file, M, \"single\"))\n",
    "\n",
    "        with open(os.path.join(pickle_path, plot_names[i] + \"_bin_info.p\"), \"wb\") as f:\n",
    "            pickle.dump(accs_confs, f)\n",
    "\n",
    "        plt.style.use('ggplot')\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(22.5, 4), sharex='col', sharey='row')\n",
    "        names = [\" (Uncal)\", \" (Temp)\", \" (Histo)\", \" (Iso)\", \" (Beta)\"]\n",
    "\n",
    "        # Uncalibrated information\n",
    "        rel_diagram_sub(bin_info_uncal[0], bin_info_uncal[1], bin_info_uncal[2], ax[0], M=M,\n",
    "                        name=\"\\n\".join(plot_names[i].split()) + names[0], xname=\"Confidence\")\n",
    "\n",
    "        for j in range(4):\n",
    "            rel_diagram_sub(accs_confs[j][k][0], accs_confs[j][k][1], accs_confs[j][k][2], ax[j + 1], M=M,\n",
    "                            name=\"\\n\".join(plot_names[i].split()) + names[j + 1], xname=\"Confidence\")\n",
    "\n",
    "        ax[0].set_ylabel(\"Accuracy\", color=\"black\")\n",
    "\n",
    "        for ax_temp in ax:\n",
    "            plt.setp(ax_temp.get_xticklabels(), rotation='horizontal', fontsize=18)\n",
    "            plt.setp(ax_temp.get_yticklabels(), fontsize=18)\n",
    "\n",
    "        plt.savefig(os.path.join(out_path, \"_\".join(plot_names[i].split()) + \".pdf\"), format='pdf', dpi=1000, bbox_inches='tight',\n",
    "                pad_inches=0.2)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [32, 100] #, 242, 376, 498]\n",
    "filter_list = [\"sample\", \"blur\", \"detail\", \"edge_enhance\", \"sharp\", \"smooth\"]\n",
    "PATH = os.path.join('..', 'logits_context')\n",
    "#files_original = [\"probs_resnet110_original_\" + f + \"_32_logits.p\" for f in filter_list]\n",
    "\n",
    "files_original = dict()\n",
    "files_blurdetail = dict()\n",
    "files_allfilter = dict()\n",
    "\n",
    "for seed in seeds:\n",
    "    #files_original[seed] = [\"probs_original_\" + f + \"_\" + str(seed) + \"_logits.p\" for f in filter_list]\n",
    "    files_blurdetail[seed] = [\"probs_resnet101_blurdetail_context_\" + f + \"_\" + str(seed) + \"_logits.p\" for f in filter_list]\n",
    "    #files_allfilter[seed] = [\"probs_allfilter_\" + f + \"_\" + str(seed) + \"_logits.p\" for f in filter_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for seed: 32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred contain different number of classes 10, 16. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1 2 3 4 5 6 7 8 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8ba6fa4414fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mplot_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ResNet-110- blurdetail \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mgen_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_blurdetail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#plot_names = [\"ResNet-110- all_filter \" + f + \"_\" + str(seed) for f in filter_list]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-80946bfa14fe>\u001b[0m in \u001b[0;36mgen_plots\u001b[0;34m(files, plot_names, M, val_set)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moriginal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"sample\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate_on_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTemperatureScaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_hb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate_on_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHistogramBinning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_iso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate_on_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsotonicRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'y_min'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_max'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7398f43374e1>\u001b[0m in \u001b[0;36mcalibrate_on_samples\u001b[0;34m(method, path, file, M, approach, m_kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_logits_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/run/media/bsl/Data/Work at Tartu/UT_resnet/calibration/cal_methods.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, logits, true)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten y_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0mmaxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m   1136\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/run/media/bsl/Data/Work at Tartu/UT_resnet/calibration/cal_methods.py\u001b[0m in \u001b[0;36m_loss_fun\u001b[0;34m(self, x, probs, true)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Calculates the loss using log-loss (cross-entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mscaled_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2265\u001b[0;31m             raise ValueError(\"y_true and y_pred contain different number of \"\n\u001b[0m\u001b[1;32m   2266\u001b[0m                              \u001b[0;34m\"classes {0}, {1}. Please provide the true \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                              \u001b[0;34m\"labels explicitly through the labels argument. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred contain different number of classes 10, 16. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1 2 3 4 5 6 7 8 9]"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "        print(\"Generating for seed: {b}\".format(b=seed))\n",
    "        #plot_names = [\"ResNet-110- original \" + f + \"_\" + str(seed) for f in filter_list]\n",
    "        #gen_plots(files_original[seed], plot_names=plot_names, val_set=False)\n",
    "\n",
    "        plot_names = [\"ResNet-101- blurdetail \" + f + \"_\" + str(seed) for f in filter_list]\n",
    "        gen_plots(files_blurdetail[seed], plot_names=plot_names, val_set=False)\n",
    "\n",
    "        #plot_names = [\"ResNet-110- all_filter \" + f + \"_\" + str(seed) for f in filter_list]\n",
    "        #gen_plots(files_allfilter[seed], plot_names=plot_names, val_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
